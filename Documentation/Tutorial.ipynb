{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "You’ll need to know a bit of Python. For a refresher, see the [Python tutorial](https://docs.python.org/3/tutorial/)\n",
    "\n",
    "### Osiris Version\n",
    "The version of Osiris is slightly different to Michael's version. This version contains the realigner built into the fReader. \n",
    "Hence please use the version of Osiris I have provided. For versions compatible to the original Osiris, consult the labnotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Version\n",
    "Python 3.x (Ensure you have a compatible version of Python 3.x installed. Python 2.x is not supported.)\n",
    "\n",
    "\n",
    "### Required Libraries and Modules\n",
    "The following libraries and modules are required to run the provided code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy hist matplotlib mplhep pandas plotly scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\O'\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_19204\\619255930.py:4: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  sys.path.insert(1, '..\\Osiris Temp\\processing\\python') # Insert your path to Osiris\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "sys.path.insert(1, '..\\Osiris Temp\\processing\\python') # Insert your path to Osiris\n",
    "import Analysis_tools as ATools\n",
    "import rawFileReader\n",
    "import proAnubis_Analysis_Tools\n",
    "import Reconstruction_tools as RTools\n",
    "import mplhep as hep\n",
    "import Timing_tools as TTools\n",
    "import Reconstruction_tools as RTools\n",
    "hep.style.use([hep.style.ATLAS])\n",
    "file_path = '../../Data/proAnubis_240716_2142.raw' # insert your file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Coverage\n",
    "\n",
    "This is a quick start tutorial example. In this tutorial you will find\n",
    "\n",
    "1. Example usage of various functions, with high level explainations\n",
    "2. How data gathered from rawfileReader, how data are processed with each other\n",
    "3. Reproducing all essential results obtained so far\n",
    "4. Comments on limitations and warnings for usage\n",
    "5. Tutorial on Timing Analysis, Reconstruction and Realigner\n",
    "\n",
    "Results including alignment metric, TDC status metric, Reconstruction, time of flight analysis, and how to integrate these systems together\n",
    "\n",
    "<span style=\"color:green\">For a documentation styled explaination for each function, see other notebook</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "After this tutorial, you should be able to understand how the data are extracted from the Raw file, how these data are then used in each function to do their designated tasks.\n",
    "\n",
    "Understand the data structure, and how to use these data outputs to plot certain plots.\n",
    "\n",
    "Using class functions and decorators to extract internal variables (Or use PyCharm...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used Data\n",
    "All example data used are latest data proAnubis_240716_2142\n",
    "The Reconstruction algorithm was adapted to the latest data behavior where noise bursts were seen, so for the sake of speed, noise bursts were ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Warning\n",
    "The code was adapted to analyse data in any situations, so the data structures were written prioritising mutability and easy extraction. This also means that for certain tasks, you might need to adjust low level functions' behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Basics\n",
    "\n",
    "The rawFileReader.py is used to read the content of .raw files that stores the data from runs of proAnubis setup. .raw files contains uncompressed binary datas that requires the rawFileReader to decode and turn into useful analysis. \n",
    "\n",
    "The Pro_Anubis is run using triggers, which triggers data taking for 1250ns if there are 4 eta side strips triggered within a fixed time frame. Each trigger is called an <span style=\"color:cyan\">event</span>. \n",
    "\n",
    "An event contains a <span style=\"color:cyan\">header</span>, which details the content of the event, then the events are written in binary <span style=\"color:cyan\">words</span>, and n <span style=\"color:cyan\">End of block</span> signals a termination of an <span style=\"color:cyan\">event</span>\n",
    "\n",
    "The <span style=\"color:cyan\">event</span> are written as `proAnubEvent` object, which contains `tdcEvent` objects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tdcEvent Class\n",
    "\n",
    "#### Overview\n",
    "The `tdcEvent` class is what the output from rawfileReaders are\n",
    "\n",
    "#### Class Variables\n",
    "\n",
    "##### `header`\n",
    "- **Type:** `Any`\n",
    "- **Description:** Represents the header word of the event. \n",
    "\n",
    "##### `words`\n",
    "- **Type:** `List[Any]`\n",
    "- **Default:** `[]`\n",
    "- **Description:** A list containing words for each event. These words represent the hits.\n",
    "\n",
    "##### `time`\n",
    "- **Type:** `Any`\n",
    "- **Description:** Holds the timing information for the event, a time stamp in Daytime format.\n",
    "\n",
    "##### `EOB`\n",
    "- **Type:** `Any`\n",
    "- **Default:** `None`\n",
    "- **Description:** Represents the end of block indicator. This is used to signify the end of a data block within a trigger.\n",
    "\n",
    "##### `qual`\n",
    "- **Type:** `int`\n",
    "- **Default:** `0`\n",
    "- **Description:** A quality check indicator. The `qual` value provides information on data integrity:\n",
    "  - If `qual == 0`, it means the data has not experienced corruption that required corrections in the raw file reader.\n",
    "  - If `qual != 0`, it indicates that data corruption occurred and was captured by the system.\n",
    "\n",
    "#### Constructor\n",
    "\n",
    "The `__init__` method initializes the `tdcEvent` class with the provided parameters:\n",
    "\n",
    "```python\n",
    "def __init__(self, header, time, words = [], eob=None, qual=0):\n",
    "    self.header = header\n",
    "    self.words = words\n",
    "    self.time = time\n",
    "    self.EOB = eob\n",
    "    self.qual = qual\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "\n",
    "To extract the data from raw files, the `get_aligned_events` function is used. At default, it will output globally aligned events until the end of the file. Usually the file is very large, hence an event loop is suggested to end early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 100/100 [00:01<00:00, 53.42Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # reload in the classs object\n",
    "order = [(0,1), (1,2), (2,3), (3,4)] # Order what you want to align\n",
    "max_process_event_chunk = 100 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "\n",
    "\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        pbar.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal variable extraction\n",
    "\n",
    "some internal variables might be interesting and worth studying. There are a variety of ways to extract them, the simplest way is to go back to the code, and add an extra class variable and record all the information in an instance. This however, is unsustainable and you shouldn't rewrite a working system just to extract some variables you will probably only require once. Usually we call it: try not to disturb a working system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorators\n",
    "Decorators are a great tool to extract and capture information within the event run time without altering the original code. please read [Decorators tutorial](https://realpython.com/primer-on-python-decorators/) for more information\n",
    "Please note you need to restart the environment if the function rewrote by the decorator runs into issue, because the function will not return to original state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example decorator for printing out the alignment insertions done without altering the original function in fReader is printed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Decorators to capture things in the loop so you don't need to redo any calculations later......\n",
    "def debug_decorator(processedEvents):\n",
    "    def inner_decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            result = func(*args, **kwargs)\n",
    "            insertion_list = result\n",
    "            update = args[0]\n",
    "            if insertion_list != [0,0,0,0,0]:\n",
    "                print(f'New alignment, Event chunk {processedEvents}, insertions {insertion_list}, Updates: {update}')\n",
    "            return result\n",
    "        return wrapper\n",
    "    return inner_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New alignment, Event chunk 302, insertions [0, 1, 1, 0, 1], Updates: [-1, 0, 1, -1]\n",
      "New alignment, Event chunk 476, insertions [1, 1, 1, 0, 0], Updates: [0, 0, 1, 0]\n",
      "New alignment, Event chunk 477, insertions [0, 0, 0, 0, 1], Updates: [0, 0, 0, -1]\n",
      "New alignment, Event chunk 516, insertions [0, 1, 1, 0, 0], Updates: [-1, 0, 1, 0]\n",
      "New alignment, Event chunk 700, insertions [0, 0, 0, 1, 0], Updates: [0, 0, -1, 1]\n",
      "New alignment, Event chunk 701, insertions [1, 0, 0, 0, 0], Updates: [1, 0, 0, 0]\n",
      "New alignment, Event chunk 830, insertions [0, 0, 0, 1, 0], Updates: [0, 0, -1, 1]\n",
      "New alignment, Event chunk 888, insertions [1, 1, 1, 0, 0], Updates: [0, 0, 1, 0]\n",
      "New alignment, Event chunk 889, insertions [0, 0, 1, 1, 1], Updates: [0, -1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#Jupyter notebook doesn't reload your import even when the content of the file is changed. This is crucial\n",
    "importlib.reload(rawFileReader)\n",
    "#The interval is the amount of chunks you want to do realignment analysis with. This also determines the size of the chunk your output is.\n",
    "interval = 100\n",
    "fReader = rawFileReader.fileReader(file_path)\n",
    "# Order is the alignment comparisons you need to specify. for each sublist in order is a pairwise comparison between 2 TDCs.\n",
    "order = [(0,1), (1,2), (2,3), (3,4)] #Your order to the TDC alignment\n",
    "# max_process_event_chunk is used to terminate early in the file reading loop. Your total number of events read will be defined as interval * max_proess_event_chunk\n",
    "max_process_event_chunk = 1000\n",
    "# some initialisation to store things\n",
    "processedEvents = 0\n",
    "events = []\n",
    "\n",
    "#Remember the original function\n",
    "original_ConstructEventInsertionList = ATools.ConstructEventInsertionList \n",
    "# condition to end event loop early\n",
    "while processedEvents < max_process_event_chunk:\n",
    "    processedEvents += 1\n",
    "    #Apply your decorator to change the function\n",
    "    ATools.ConstructEventInsertionList  = debug_decorator(processedEvents)(original_ConstructEventInsertionList)\n",
    "    event = fReader.get_aligned_events(order=order, interval=interval)\n",
    "#reset afterwards\n",
    "ATools.ConstructEventInsertionList  = original_ConstructEventInsertionList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably have noticed that we are constructing the insertion list and inserting the required amount of insertions in each TDC. This was done to ensure we don't over insert events, although this isn't really a problem with the current iteration, but if a large insertion is needed for each content of your order, you could end up with a very long list of empty events, which causes the next chunk to not perform nearly as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re_calculation\n",
    "\n",
    "Athough some calculations are processed internally, it might be easier to reprocess the output event list again by redoing the necessary calculations. This will result in redundant calculations, and not possible for all calculations, such as TDC monitoring metric and realignment words used. \n",
    "\n",
    "Both fReader and this code is written to ensure the data is calculated in chunks, this avoids memory overflow, and allow the program to start and terminate anywhere one desires.\n",
    "\n",
    "When the event chunk is loaded in to the instance, calculation has to be done within this instance, and saved into an external variable. Below is an example of using the function RTools.`find_tdc_event_count` to calculate the number of headers in each TDC and writing into a buffer `tdc_event_count_buffer` for that instance, then the information from the buffer is tranfered into an external memory storage `tdc_event_count` outside the process loop\n",
    "\n",
    "Similarly, the alignment metric can be calculated from ATools.`calcAvgAlign` (for more information refer to documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 100/100 [00:02<00:00, 46.74Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # reload in the classs object\n",
    "order = [(0,1), (1,2), (2,3), (3,4)] # Order what you want to align\n",
    "max_process_event_chunk = 100 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "\n",
    "#Initialise variables to store the results\n",
    "mets = [[] for tdc in range(5)]\n",
    "empty_header = 0 # Counting the number of empty headers\n",
    "emtpy_events_with_header = [[], []] # prepare two lists of same dimensions for line graph plotting\n",
    "tdc_event_count = [[] for tdc in range(5)]# prepare for histogram plotting\n",
    "\n",
    "\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the aligned events\n",
    "        tdc_event_count_buffer = RTools.find_tdc_event_count(event_chunk) # finding the number of headers\n",
    "        [tdc_event_count[i].append(tdc_event_count_buffer[i]) for i in range(5)] # write from buffer to memory\n",
    "        for idx, (i, j) in enumerate(order):\n",
    "            x, y, l, m = ATools.find_tdc_alignment_metric(i, j) # determining which RPCs to use for aignment metric\n",
    "            alignMet = ATools.calcAvgAlign(event_chunk, 0, x, y, l, m, i, j, processedEvents) # determine the alignment metric\n",
    "            empty_header += (alignMet == -1) # Calculating the number of empty headers\n",
    "            emtpy_events_with_header[0].append(empty_header) # write to memory\n",
    "            emtpy_events_with_header[1].append(processedEvents)\n",
    "            mets[idx].append(alignMet) # write to memory\n",
    "        pbar.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_22812\\4261640906.py:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  ax.set_ylabel('Average $\\sqrt{d\\eta^2+d\\phi^2}$')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for idx, item in enumerate(order):\n",
    "    met = mets[idx]\n",
    "    i, j = item\n",
    "    binsx = [x for x in range(len(met))]\n",
    "    ax.plot(binsx, met, label=f'TDC{i} and TDC{j}, offset 0')\n",
    "\n",
    "\n",
    "ax.set_xlim(0, max_process_event_chunk)\n",
    "ax.set_ylim(-1, 40)\n",
    "ax.legend()\n",
    "ax.set_title('Alignment graph')\n",
    "ax.set_ylabel('Average $\\sqrt{d\\eta^2+d\\phi^2}$')\n",
    "ax.set_xlabel('Processed Event chunks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(emtpy_events_with_header[1], emtpy_events_with_header[0], marker='o')\n",
    "plt.xlabel('Processed Events')\n",
    "plt.ylabel('Only Header')\n",
    "plt.title('Only Header vs Processed Events')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for tdc in range(5):\n",
    "    met = tdc_event_count[tdc]\n",
    "    binsx = [x for x in range(len(met))]\n",
    "    ax.plot(binsx, met, label=f'TDC{tdc}')\n",
    "\n",
    "ax.set_xlim(0, max_process_event_chunk)\n",
    "# ax.set_ylim(-1, 100)\n",
    "ax.legend()\n",
    "ax.set_title('TDC number of events recorded')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Processed Event')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "fReader doesn't have many instances for storing metric, they are typically thrown out after use, hence it is very difficult to extract these out. However, it becomes easier as we move to Timing Analyser and Reconstructor where the class instances can be called directly to extract and record data. \n",
    "\n",
    "The general idea is, if the data is in Osiris, they are NOT stored so that it will run smoothly always. Other data like reconstructio and timing analyser will be stored in class instances, which will remain in memory as long you as you don't reload them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `Timing Analyser`\n",
    "\n",
    "The `Timing_Analyser` class is designed to process and analyze timing data from event chunks. This class helps in calculating and visualizing time of flight analysis, residuals, and other related metrics for the pro_anubis detectors. It provides functionalities to update events, read TDC (Time-to-Digital Converter) time differences, calculate residuals, check eta trigger, and plot various data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using any class from `proAnubis_analysis_tools`, it is important that a new event loop format is used. The loop involves an initialisation, which clears all instances. When the event loop is entered, calculations are done through internal instances, meaning one must not TAnalyser, but to update the event_chunk through TAnalyser.`update_event`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most useful metric to determine the corruption status is TAnalyser.`check_eta_trigger`. This function checks if each chunk has corruption and also count the total number of corrupted events with which RPC was involved in the corruption within the class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 150/150 [00:03<00:00, 49.90Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 150 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "TAnalyser = proAnubis_Analysis_Tools.Timing_Analyser(initial_event_chunk,processedEvents)\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        if event_chunk:\n",
    "            TAnalyser.update_event(event_chunk, processedEvents)\n",
    "            status, failure = TAnalyser.check_eta_trigger()\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot these on a graph, note we are calling from TAnalyser to obtain the information we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_counts_windows = [len(TAnalyser.count[count]) for count in range(7)]\n",
    "total_windows = sum(event_counts_windows)\n",
    "normalized_windows = [count / total_windows for count in event_counts_windows]\n",
    "normalized_windows.append(normalized_windows[-1])\n",
    "plt.figure(figsize=(12, 8))\n",
    "r1 = list(range(7)) + [6.5]\n",
    "\n",
    "# plt.step(r1, normalized_linux, color='mediumseagreen', linestyle='-', linewidth=2, markersize=6, label='0-15000', alpha=1, where='mid')\n",
    "plt.step(r1, normalized_windows, color='dodgerblue', linestyle='-', linewidth=2, markersize=6, label='0-15000?', alpha=1, where='mid')\n",
    "\n",
    "plt.xlabel('Trigger Number')\n",
    "plt.ylabel('Number of Events Normalized')\n",
    "plt.title('Normalized Event Count')\n",
    "plt.ylim(0)\n",
    "plt.xlim(0, 6.5)\n",
    "plt.xticks(range(7))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, one can also plot directly from class accessing the class variable. This makes no difference fundamentally, but it does make the code much clearer and immediately clear what you are plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAnalyser.plot_rpc_involvement_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDC monitoring\n",
    "The TDC monitoring script is innatly written in the fReader. Every 2500 events, the TDC status is monitored. This TDC status metric counts the number of events where the first time is within the \"bad region\" with an event time> 300 ns compare to the \"good region\" where the event time <300ns. It was typicaly found that the TDC is in an error state when the fraction is larger than 0.2\n",
    "\n",
    "As a complementary output, TDC_info is also outputted to show the first hit time and first hit channels on each TDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 30/30 [00:00<00:00, 31.98Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 30 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "#Initialisation\n",
    "tdc_mets = [[] for tdc in range(5)]\n",
    "Tot_TDC_info = [[] for tdc in range(5)]\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk, tdc_met, TDC_info = fReader.get_aligned_events(order=order, interval=interval, extract_tdc_mets = True) \n",
    "        # get_aligned_events have a choice to output the tdc metric and tdc information by setting extract_tdc_mets to be True\n",
    "        [tdc_mets[i].append(tdc_met[i]) for i in range(5) if tdc_met[i] != 0]\n",
    "        [Tot_TDC_info[i].extend(TDC_info[i]) for i in range(5) if TDC_info[i]]\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for tdc in range(5):\n",
    "    met = tdc_mets[tdc]\n",
    "    binsx = [x * 25 for x in range(len(met))]\n",
    "    ax.plot(binsx,met, label = f'tdc {tdc}', color = colors[tdc])\n",
    "\n",
    "ax.set_xlim(0,max_process_event_chunk)\n",
    "# ax.set_ylim(0,1)\n",
    "ax.legend()\n",
    "ax.set_title('TDC monitoring metric')\n",
    "ax.set_ylabel('bad time behavior / nominal time behavior')\n",
    "ax.set_xlabel('processed Event')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting these out, one will see the first hit time and first hit channels vary between the good and bad regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(TTools)\n",
    "TTools.plot_tdc_error_times(Tot_TDC_info)\n",
    "TTools.plot_tdc_error_times_custom_ranges(Tot_TDC_info, [(0, 100), (100, 200)], output_pdf='Data_output/TDC_first_hit_time.pdf')\n",
    "TTools.plot_tdc_error_channels(Tot_TDC_info)\n",
    "TTools.plot_tdc_error_channels_custom_ranges(Tot_TDC_info, [(0, 100), (100, 200)], tdcs_to_plot=None, output_pdf='Data_output/TDC_first_hit_time_channel.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the variables used to calculate the alignment metric is even more difficult, since the output data is not representative to what the code used to compute the alignment metric, hence we need decorators to capture the variable in question during the program's run time, record them, and plot them. Below is an example decorator designed to capture the event words used when doing realignments. \n",
    "\n",
    "Essentially it access the function in question, capturing its' input and output, and calculating the the min channels after the original calculation. \n",
    "\n",
    "You may ask why do I do this, well this is because the functino calculating this is step 4 deep in the code, you need to pretty much add this extra check in every functino it passes through, eventually extracting it to the top level. And you probably shouldn't mess with fReader that much.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capturer:\n",
    "    def __init__(self):\n",
    "        self.TDC_alignment_time = [[] for _ in range(5)]\n",
    "        self.processedEvents = 0 \n",
    "\n",
    "    def extra_calculation_decorator(self, func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            minTimes = [300, 300]\n",
    "            minChans = [-1, -1]\n",
    "            minRPC = [-1, -1]\n",
    "            minWord = [-1, -1]\n",
    "            tdc = [-1, -1]\n",
    "            eta = [True, True]\n",
    "            \n",
    "            rpc1Hits = args[0]\n",
    "            rpc2Hits = args[1]\n",
    "            skipChans = kwargs.get('skipChans', [])\n",
    "\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            if result == -1:\n",
    "                return result\n",
    "            for hit in rpc1Hits:\n",
    "                if hit.time < minTimes[0] and hit.channel not in skipChans:\n",
    "                    minTimes[0] = hit.time\n",
    "                    minChans[0] = hit.channel\n",
    "                    minRPC[0] = hit.rpc\n",
    "                    eta[0] = hit.eta\n",
    "            for hit in rpc2Hits:\n",
    "                if hit.time < minTimes[1] and hit.channel not in skipChans:\n",
    "                    minTimes[1] = hit.time\n",
    "                    minChans[1] = hit.channel\n",
    "                    minRPC[1] = hit.rpc\n",
    "                    eta[1] = hit.eta\n",
    "            \n",
    "            if -1 in minChans:\n",
    "                return -1\n",
    "            \n",
    "            a = TTools.rpcHitToTdcChan(minRPC[0], minChans[0], eta[0])\n",
    "            b = TTools.rpcHitToTdcChan(minRPC[1], minChans[1], eta[1])\n",
    "            \n",
    "            tdc[0], minWord[0] = a\n",
    "            tdc[1], minWord[1] = b\n",
    "\n",
    "            self.TDC_alignment_time[tdc[0]].append((minTimes[0], a, self.processedEvents))\n",
    "            self.TDC_alignment_time[tdc[1]].append((minTimes[1], b, self.processedEvents))\n",
    "\n",
    "            return result\n",
    "\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 200/200 [00:03<00:00, 59.52Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(TTools)\n",
    "\n",
    "Capturer = Capturer()\n",
    "\n",
    "# Apply the decorator\n",
    "original_testAlign = ATools.testAlign\n",
    "ATools.testAlign = Capturer.extra_calculation_decorator(ATools.testAlign)\n",
    "\n",
    "# Main loop\n",
    "interval = 100  # Set your monitoring chunk size\n",
    "fReader = rawFileReader.fileReader(file_path)  # Load in the class object\n",
    "order = [[0, 1], [1, 2], [2, 3], [3, 4]]  # Order what you want to align\n",
    "max_process_event_chunk = 200  # End the loop early\n",
    "processedEvents = 0  # Initialization\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "tdc_mets = [[] for tdc in range(5)]\n",
    "Tot_TDC_info = [[] for tdc in range(5)]\n",
    "\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        Capturer.processedEvents = processedEvents\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        pbar.update(1)\n",
    "# Return to original, or restart kernal\n",
    "ATools.testAlign = original_testAlign\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence you can find which channels and which timing are used for alignment metric calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(TTools)\n",
    "TTools.plot_tdc_alignment_channels_custom_ranges(Capturer.TDC_alignment_time, [(0, 50), (50, 100)], output_pdf='Data_output/TDC_alignment_channels_used.pdf' )\n",
    "TTools.plot_tdc_alignment_times_custom_ranges(Capturer.TDC_alignment_time, [(0, 50), (50, 100)], output_pdf='Data_output/TDC_alignment_time_used.pdf' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time walk effect\n",
    "\n",
    "It was found that signals takes time to travel from the FEB to the TDC, and on top of that, a systematic timing offset on each channel was also observed. Below is a tool written by Michael to read the timing difference between eta and phi channels, and averaging them across the entire chunk, and finally plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 1000 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "TAnalyser = proAnubis_Analysis_Tools.Timing_Analyser(initial_event_chunk,processedEvents)\n",
    "while processedEvents < max_process_event_chunk:\n",
    "    processedEvents += 1\n",
    "    event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "    if event_chunk:\n",
    "        TAnalyser.update_event(event_chunk, processedEvents)\n",
    "        TAnalyser.readTDCTimeDiffs()\n",
    "        \n",
    "outDict = {'totDiffs':TAnalyser.totDiffs,\n",
    "                    'nDiffs':TAnalyser.nDiffs,\n",
    "                    'diffHists':TAnalyser.diffHists} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual is calculated by applying the time walk correction, then averaging across the whole strip to a 2D plane. You should be able to see the time walk effect gone after applying the correction, and we are left with systematic corrections only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n"
     ]
    }
   ],
   "source": [
    "residEta, residPhi = TAnalyser.Calculate_Residual_and_plot_TDC_Time_Diffs( \n",
    "                                                     pdf_filename='Data_output/TDC_time_diffs.pdf', \n",
    "                                                     max_itr = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at TAnalyser.`Calculate_Residual_and_plot_TDC_Time_Diffs`, there is a magic number for slope and offset, which is curve fitted by the function below. feel free to change it after fitting a larger amount of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15415730778671458 16.159064658832435\n",
      "R² value: 0.6248481250576245\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "TAnalyser.plot_and_fit_tof_corrections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at each individual strip, one can use this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(TTools)\n",
    "TTools.show_strip_time_info(outDict, 20, 22, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot the residual through this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "TAnalyser.plot_residual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The Timing analysis class contains all the functions needed to analyse the time walk effect, capturing timing corruptions in TDCs, and also monitoring tdcs as well. A general idea is that anything designed to run smoothly to produce the most useful result is written in TAnalyser with all instances stored internally. Analysis on individual bits will be in TTools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstructor\n",
    "Very similar to the Timing Analyser, the Reconstructor's main goal is to reconstruct muon paths from the given data. The details of the code can be found under the documentation under the function RTools.`reconstruct_timed_Chi2_ByRPC`\n",
    "\n",
    "below is an example of finding the efficiency of each RPC. The test RPC is removed, and a line is reconstructed using other 5 RPCs. This line is then extrapolated to the test RPCs' location, and a area of certain radius called the `tolerance` is probed for the hits. the success and failure together with their location is recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 400/400 [01:49<00:00,  3.65Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 400 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "reconstructor = proAnubis_Analysis_Tools.Reconstructor(initial_event_chunk, processedEvents, tolerance = [i / 5 for i in range(30)])\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        #Zone of Reconstruction\n",
    "        if event_chunk:\n",
    "            # We need to update the event like TAnalyser as well\n",
    "            reconstructor.update_event(event_chunk, processedEvents)\n",
    "            # populate_hits turns TDC bit wise information into their corresponding strips\n",
    "            reconstructor.populate_hits()\n",
    "            # This is obtionnal, and requires the residual of eta and phi\n",
    "            reconstructor.apply_systematic_correction(residEta, residPhi)\n",
    "            # make_cluster does temporal and spatial coincidence between the stips, and reconstruction is done\n",
    "            cluster = reconstructor.make_cluster()\n",
    "            # Finally, recontruction is done using cluster information\n",
    "            reconstructor.reconstruct_and_extrapolate(cluster)\n",
    "            # This is for a test I am doing, you can ignore it for now Jonas\n",
    "            reconstructor.append_efficiency_to_memory()\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructor.plot_efficiency_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2155, 2117, 2706, 2557, 3187, 2369]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "for RPC in range(6):\n",
    "    if reconstructor.possible_reconstructions[RPC] == 0:\n",
    "        efficiency = [0 for x in reconstructor.successful_reconstructions[RPC]]\n",
    "    else:\n",
    "        efficiency = [x / reconstructor.possible_reconstructions[RPC] for x in reconstructor.successful_reconstructions[RPC]]\n",
    "    plt.plot(reconstructor.tol, efficiency, label=f'RPC {RPC}')\n",
    "\n",
    "plt.xlabel('Tolerance')\n",
    "plt.ylabel('Efficiency')\n",
    "plt.title('idc what the title is')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(reconstructor.possible_reconstructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot a heat map using the information obtained from the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "PDF created successfully.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "rpcNames = {0: \"Triplet Low\", 1: \"Triplet Mid\", 2: \"Triplet Top\", 3: \"Singlet\", 4: \"Doublet Low\", 5: \"Doublet Top\"}\n",
    "\n",
    "success_events = [[0 for etchan in range(32)] for phchan in range(64)]\n",
    "\n",
    "with PdfPages('Data_output/reconstruction_heatmap_plots.pdf') as pdf:\n",
    "    for rpc in range(6):\n",
    "        for ph in range(64):\n",
    "            for et in range(32):\n",
    "                if reconstructor.successful_reconstructed_coords[rpc][ph][et] > 0:\n",
    "                    total_successful = reconstructor.successful_reconstructed_coords[rpc][ph][et]\n",
    "                    total_events = reconstructor.possible_reconstructions_coords[rpc][ph][et]\n",
    "                    if total_events > 0:\n",
    "                        success_events[ph][et] = total_successful / total_events\n",
    "                    else:\n",
    "                        success_events[ph][et] = 0  # No events, efficiency is 0\n",
    "\n",
    "        fig, ax = plt.subplots(1, figsize=(16, 8), dpi=100)\n",
    "        etachannels = [x - 0.5 for x in range(33)]\n",
    "        phichannels = [x - 0.5 for x in range(65)]\n",
    "        etaHist = (success_events, np.array(phichannels), np.array(etachannels))\n",
    "        zrange = [0, max(max(row) for row in success_events)]\n",
    "        thisHist = hep.hist2dplot(etaHist, norm=colors.Normalize(zrange[0], zrange[1]))\n",
    "        thisHist.cbar.set_label('Successful reconstructions / Possible reconstructions', rotation=270, y=0.3, labelpad=23)\n",
    "        plt.ylim(31.5, -0.5)\n",
    "        plt.ylabel(\"Eta Channel\")\n",
    "        plt.xlabel(\"Phi Channel\")\n",
    "        ax.set_title(rpcNames[rpc])\n",
    "\n",
    "        # Draw lines\n",
    "        x_points = [-0.5, 64.5]\n",
    "        y_points = [7.5, 15.5, 23.5]\n",
    "        for y_point in y_points:\n",
    "            plt.plot(x_points, [y_point, y_point], 'k', linestyle='dotted')\n",
    "        y_points = [-0.5, 31.5]\n",
    "        x_points = [7.5, 15.5, 23.5, 31.5, 39.5, 47.5, 55.5]\n",
    "        for x_point in x_points:\n",
    "            plt.plot([x_point, x_point], y_points, 'k', linestyle='dashed')\n",
    "\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"PDF created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting angle information\n",
    "\n",
    "One side product of reconstruction algorithm is the extraction of angles. This uses all 6 RPCs to reconstruct tracks, and find their angular distribution. The reason why a separate function is used to the efficiency calculation will be made clear later in the detailed explaination section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 150/150 [00:17<00:00,  8.38Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 150 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "reconstructor = proAnubis_Analysis_Tools.Reconstructor(initial_event_chunk, processedEvents, tof_correction=True)\n",
    "TAnalyser = proAnubis_Analysis_Tools.Timing_Analyser(initial_event_chunk,processedEvents)\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        if event_chunk:\n",
    "            reconstructor.update_event(event_chunk, processedEvents)\n",
    "            # if processedEvents < 250:\n",
    "            #     pbar.update(1)\n",
    "            #     continue\n",
    "            reconstructor.populate_hits()\n",
    "            reconstructor.apply_systematic_correction(residEta, residPhi)\n",
    "            cluster = reconstructor.make_cluster()\n",
    "            filtered_events = RTools.filter_events(cluster,1,6)     \n",
    "            reconstructor.extract_angles_phi_eta_timed_DZ_modified(filtered_events)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was about to make this into a function, but then i need to write documentation for a code purely for plotting, might as well write it out here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(14, 20))\n",
    "bin_edges = np.arange(-90.5, 91.5, 1)\n",
    "phi_edges = np.arange(-180.5, 181.5, 1)\n",
    "ax1.bar(bin_edges[:-1], reconstructor.eta_histogram, width=1, edgecolor='black', align='edge')\n",
    "ax1.set_title('eta Angles Histogram chunk3')\n",
    "ax1.set_xlabel('eta Angle (degrees)')\n",
    "ax1.set_ylabel('Counts')\n",
    "\n",
    "ax2.bar(bin_edges[:-1], reconstructor.phi_histogram, width=1, edgecolor='black', align='edge')\n",
    "ax2.set_title('phi Angles Histogram chunk3')\n",
    "ax2.set_xlabel('phi Angle (degrees)')\n",
    "ax2.set_ylabel('Counts')\n",
    "\n",
    "ax3.bar(phi_edges[:-1], reconstructor.solid_theta_histogram, width=1, edgecolor='black', align='edge')\n",
    "ax3.set_title('solid theta Angles Histogram chunk3')\n",
    "ax3.set_xlabel('solid theta Angle (degrees)')\n",
    "ax3.set_ylabel('Counts')\n",
    "\n",
    "ax4.bar(phi_edges[:-1], reconstructor.solid_phi_histogram, width=1, edgecolor='black', align='edge')\n",
    "ax4.set_title('solid phi Angles Histogram chunk3')\n",
    "ax4.set_xlabel('solid phi Angle (degrees)')\n",
    "ax4.set_ylabel('Counts')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time of Flight analysis\n",
    "\n",
    "Now we can do the recontruction and find the time of flight information across RPCs to determine our time resolution and also potential errors. This is another function called `reconstruct_and_findtof`, which records the hit time difference of the reconstructed paths by specifying which set of RPC one needs to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 150/150 [00:24<00:00,  6.05Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "rpc_comparison = [[0,1], [0,2], [0,3], [0,4], [0,5]]\n",
    "max_process_event_chunk = 150 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "reconstructor = proAnubis_Analysis_Tools.Reconstructor(initial_event_chunk, processedEvents, tof_correction=True)\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        if event_chunk:\n",
    "            reconstructor.update_event(event_chunk, processedEvents)\n",
    "            # if processedEvents < 250:\n",
    "            #     pbar.update(1)\n",
    "            #     continue\n",
    "            reconstructor.populate_hits()\n",
    "            reconstructor.apply_systematic_correction(residEta, residPhi)\n",
    "            cluster = reconstructor.make_cluster()\n",
    "            reconstructor.reconstruct_and_findtof(cluster, rpc_comparisons=rpc_comparison)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\OneDrive - University of Cambridge\\Desktop\\summer2\\Documentation\\Reconstruction_tools.py:11: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  sys.path.insert(1, 'Osiris Temp\\processing\\python')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid average value for RPC0-5: 1.1988900362149975\n",
      "Gaussian fit parameters for RPC[0, 1]: amplitude = 49.381479909205886, mean = 1.0273674458394983, std deviation = 1.0679953129059105\n",
      "Mid average value for RPC1-5: 0.9242464286135823\n",
      "Gaussian fit parameters for RPC[0, 2]: amplitude = 51.73059772838432, mean = 0.6851203461356671, std deviation = -1.031793000500714\n",
      "Mid average value for RPC2-5: 3.4430684151142663\n",
      "Gaussian fit parameters for RPC[0, 3]: amplitude = 48.25755059952136, mean = 3.1513194907531163, std deviation = 1.0048204005991517\n",
      "Mid average value for RPC3-5: 5.71393662844103\n",
      "Gaussian fit parameters for RPC[0, 4]: amplitude = 43.31381369825579, mean = 6.724989114447141, std deviation = 1.0977647179354555\n",
      "Mid average value for RPC4-5: 5.347299454410857\n",
      "Gaussian fit parameters for RPC[0, 5]: amplitude = 42.015526902022536, mean = 5.758250471389722, std deviation = 1.1522278002826838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data_output/tof.pdf'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(RTools)\n",
    "RTools.compile_and_plot_tof(reconstructor.dT,rpc_comparison, pdf_filename='Data_output/tof.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\OneDrive - University of Cambridge\\Desktop\\summer2\\Documentation\\Reconstruction_tools.py:11: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  sys.path.insert(1, 'Osiris Temp\\processing\\python')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid average value for RPC[0, 1], chunk1: 0.8787143954183173\n",
      "Gaussian fit parameters for RPC0-1: amplitude = 5.4110226680105455, mean = 0.837397167418056, std deviation = -0.9509791571884038\n",
      "Mid average value for RPC[0, 1], chunk2: 1.3453856035766494\n",
      "Gaussian fit parameters for RPC0-2: amplitude = 6.2783667987945, mean = 1.0159418789281054, std deviation = -0.7971999116915058\n",
      "Mid average value for RPC[0, 1], chunk3: 1.129327343008428\n",
      "Gaussian fit parameters for RPC0-3: amplitude = 5.47113860763438, mean = 1.292520661119769, std deviation = -0.9519835821941943\n",
      "Mid average value for RPC[0, 1], chunk4: 1.0984937404001336\n",
      "Gaussian fit parameters for RPC0-4: amplitude = 5.143038128419009, mean = 1.0826311959706776, std deviation = -1.0120360919491358\n",
      "Mid average value for RPC[0, 1], chunk5: 1.1660957565742778\n",
      "Gaussian fit parameters for RPC0-5: amplitude = 4.859664672491032, mean = 1.1032570085016473, std deviation = 1.1170097673576056\n",
      "Mid average value for RPC[0, 1], chunk6: 1.2199430686291637\n",
      "Gaussian fit parameters for RPC0-6: amplitude = 4.521964344538038, mean = 0.9699351172166414, std deviation = -1.2149069870653553\n",
      "Mid average value for RPC[0, 1], chunk7: 1.311586377430841\n",
      "Gaussian fit parameters for RPC0-7: amplitude = 4.173463414019191, mean = 1.009593970776228, std deviation = 1.2437254533701176\n",
      "Mid average value for RPC[0, 1], chunk8: 1.350083232181585\n",
      "Gaussian fit parameters for RPC0-8: amplitude = 5.295208207733258, mean = 1.0268362641617184, std deviation = -0.9113586671308279\n",
      "Mid average value for RPC[0, 1], chunk9: 1.0930582403859572\n",
      "Gaussian fit parameters for RPC0-9: amplitude = 5.791974591131905, mean = 0.7480240968223376, std deviation = 0.8033288784491307\n",
      "Mid average value for RPC[0, 1], chunk10: 1.4406242656680643\n",
      "Gaussian fit parameters for RPC0-10: amplitude = 4.26345246129594, mean = 1.0808841111003118, std deviation = -1.266580455896671\n",
      "Mid average value for RPC[0, 2], chunk1: 0.7040000094280597\n",
      "Gaussian fit parameters for RPC1-1: amplitude = 4.533151432516212, mean = 0.7141068230031851, std deviation = 1.2122963472324553\n",
      "Mid average value for RPC[0, 2], chunk2: 1.00555373753712\n",
      "Gaussian fit parameters for RPC1-2: amplitude = 6.062041784722359, mean = 0.6937309570918653, std deviation = -0.8596730523417295\n",
      "Mid average value for RPC[0, 2], chunk3: 0.8376127816693307\n",
      "Gaussian fit parameters for RPC1-3: amplitude = 5.05401792134967, mean = 0.8121709127929796, std deviation = -1.0786773049892966\n",
      "Mid average value for RPC[0, 2], chunk4: 0.8150587284924679\n",
      "Gaussian fit parameters for RPC1-4: amplitude = 5.6015126914212985, mean = 0.8525342367167744, std deviation = -0.9515709957575975\n",
      "Mid average value for RPC[0, 2], chunk5: 0.9363585155457829\n",
      "Gaussian fit parameters for RPC1-5: amplitude = 4.431361101573476, mean = 0.7228862529194556, std deviation = -1.2281645957516827\n",
      "Mid average value for RPC[0, 2], chunk6: 0.8622215961130504\n",
      "Gaussian fit parameters for RPC1-6: amplitude = 5.004815605303564, mean = 0.568945372564068, std deviation = -1.088766700510993\n",
      "Mid average value for RPC[0, 2], chunk7: 1.1357376982318246\n",
      "Gaussian fit parameters for RPC1-7: amplitude = 5.610295226528597, mean = 0.6252902647309262, std deviation = -0.8877713212040684\n",
      "Mid average value for RPC[0, 2], chunk8: 1.1246735075754606\n",
      "Gaussian fit parameters for RPC1-8: amplitude = 5.118757767894883, mean = 0.6141958535090466, std deviation = -1.0267881914023935\n",
      "Mid average value for RPC[0, 2], chunk9: 0.6277558476138543\n",
      "Gaussian fit parameters for RPC1-9: amplitude = 5.25419660776131, mean = 0.5999515037876715, std deviation = -1.0044461227892458\n",
      "Mid average value for RPC[0, 2], chunk10: 1.1985401597090835\n",
      "Gaussian fit parameters for RPC1-10: amplitude = 5.386865149748653, mean = 0.6321040194355115, std deviation = -0.9584382529368074\n",
      "Mid average value for RPC[0, 3], chunk1: 1.9138286887140012\n",
      "Gaussian fit parameters for RPC2-1: amplitude = 2.144545985291012, mean = 2.339014253908959, std deviation = 2.5304467700401303\n",
      "Mid average value for RPC[0, 3], chunk2: 3.5067909751273048\n",
      "Gaussian fit parameters for RPC2-2: amplitude = 4.365776050425147, mean = 2.962973003073849, std deviation = 0.9048551650432882\n",
      "Mid average value for RPC[0, 3], chunk3: 3.1268841320298293\n",
      "Gaussian fit parameters for RPC2-3: amplitude = 4.961655489186266, mean = 3.3971163838882563, std deviation = 1.0548162976770847\n",
      "Mid average value for RPC[0, 3], chunk4: 2.7254846678680247\n",
      "Gaussian fit parameters for RPC2-4: amplitude = 4.670890554943102, mean = 3.34983710335678, std deviation = 1.0678510677142725\n",
      "Mid average value for RPC[0, 3], chunk5: 3.859685761489394\n",
      "Gaussian fit parameters for RPC2-5: amplitude = 5.807175502017767, mean = 3.178582559842308, std deviation = 0.8929212038712734\n",
      "Mid average value for RPC[0, 3], chunk6: 3.8410408936109204\n",
      "Gaussian fit parameters for RPC2-6: amplitude = 5.569513616927376, mean = 2.8449318485115445, std deviation = 0.8811028787263188\n",
      "Mid average value for RPC[0, 3], chunk7: 4.0663811208930065\n",
      "Gaussian fit parameters for RPC2-7: amplitude = 4.357396792135329, mean = 3.3900553461038205, std deviation = 1.1416820884405452\n",
      "Mid average value for RPC[0, 3], chunk8: 3.5296864446898137\n",
      "Gaussian fit parameters for RPC2-8: amplitude = 6.406450466457148, mean = 3.160502270396229, std deviation = 0.7135841753291048\n",
      "Mid average value for RPC[0, 3], chunk9: 3.9004145019299186\n",
      "Gaussian fit parameters for RPC2-9: amplitude = 5.1092128247684325, mean = 3.0662769030574575, std deviation = 1.0023821504101962\n",
      "Mid average value for RPC[0, 3], chunk10: 3.9577939729502063\n",
      "Gaussian fit parameters for RPC2-10: amplitude = 5.753644627392687, mean = 3.0952290879293805, std deviation = 0.8166674178805013\n",
      "Mid average value for RPC[0, 4], chunk1: 3.5993055411003527\n",
      "Gaussian fit parameters for RPC3-1: amplitude = 3.101593763017183, mean = 6.193279322643094, std deviation = 1.0819872370882821\n",
      "Mid average value for RPC[0, 4], chunk2: 4.641442739929755\n",
      "Gaussian fit parameters for RPC3-2: amplitude = 4.170284922894981, mean = 6.817193820136887, std deviation = 0.9359943912868196\n",
      "Mid average value for RPC[0, 4], chunk3: 6.202359049363107\n",
      "Gaussian fit parameters for RPC3-3: amplitude = 4.915068523859538, mean = 6.727903932485739, std deviation = -1.0225591770638112\n",
      "Mid average value for RPC[0, 4], chunk4: 5.946136679929751\n",
      "Gaussian fit parameters for RPC3-4: amplitude = 5.03806573077592, mean = 7.086067601756717, std deviation = 0.9047639358963065\n",
      "Mid average value for RPC[0, 4], chunk5: 5.756935179142148\n",
      "Gaussian fit parameters for RPC3-5: amplitude = 5.5261294643418335, mean = 6.935765104691811, std deviation = -0.8788754774603281\n",
      "Mid average value for RPC[0, 4], chunk6: 5.867552578879089\n",
      "Gaussian fit parameters for RPC3-6: amplitude = 3.803935003204147, mean = 6.437117051767166, std deviation = 1.4214138374257141\n",
      "Mid average value for RPC[0, 4], chunk7: 5.810103427491246\n",
      "Gaussian fit parameters for RPC3-7: amplitude = 4.211305025554643, mean = 6.541614996082778, std deviation = 1.1967611705447656\n",
      "Mid average value for RPC[0, 4], chunk8: 5.794200637158148\n",
      "Gaussian fit parameters for RPC3-8: amplitude = 4.557493724840543, mean = 6.727907567310583, std deviation = 1.058304648994646\n",
      "Mid average value for RPC[0, 4], chunk9: 6.290746225668131\n",
      "Gaussian fit parameters for RPC3-9: amplitude = 4.406467425485173, mean = 6.743935271552181, std deviation = -1.1970350134660184\n",
      "Mid average value for RPC[0, 4], chunk10: 7.064680841450163\n",
      "Gaussian fit parameters for RPC3-10: amplitude = 5.730374092340062, mean = 6.673197022065157, std deviation = -0.7882278979353421\n",
      "Mid average value for RPC[0, 5], chunk1: 2.868205292599833\n",
      "Gaussian fit parameters for RPC4-1: amplitude = 3.051730441176784, mean = 5.459016951991076, std deviation = 1.1975158623414406\n",
      "Mid average value for RPC[0, 5], chunk2: 5.031612346380177\n",
      "Gaussian fit parameters for RPC4-2: amplitude = 3.445713019100676, mean = 5.943996753833149, std deviation = 1.1882866144467217\n",
      "Mid average value for RPC[0, 5], chunk3: 5.479001501566249\n",
      "Gaussian fit parameters for RPC4-3: amplitude = 4.1619211826968225, mean = 5.554948076625697, std deviation = 1.2501955512889888\n",
      "Mid average value for RPC[0, 5], chunk4: 5.043731250363024\n",
      "Gaussian fit parameters for RPC4-4: amplitude = 4.607344629014207, mean = 5.904804394947945, std deviation = 1.0335487224584556\n",
      "Mid average value for RPC[0, 5], chunk5: 5.861461326851802\n",
      "Gaussian fit parameters for RPC4-5: amplitude = 4.104455343972606, mean = 5.914352051342541, std deviation = 1.2247527774664084\n",
      "Mid average value for RPC[0, 5], chunk6: 5.327597183775303\n",
      "Gaussian fit parameters for RPC4-6: amplitude = 5.570113436782218, mean = 5.52756900979262, std deviation = 0.8755328919626132\n",
      "Mid average value for RPC[0, 5], chunk7: 5.79101133508219\n",
      "Gaussian fit parameters for RPC4-7: amplitude = 4.198093686399377, mean = 5.749816473061721, std deviation = 1.1681402084558639\n",
      "Mid average value for RPC[0, 5], chunk8: 5.696124677656652\n",
      "Gaussian fit parameters for RPC4-8: amplitude = 4.747154059005033, mean = 5.850946557634407, std deviation = 1.0227200853720855\n",
      "Mid average value for RPC[0, 5], chunk9: 5.570215862880335\n",
      "Gaussian fit parameters for RPC4-9: amplitude = 4.85570098726154, mean = 5.7096125097249315, std deviation = -1.0992111864834968\n",
      "Mid average value for RPC[0, 5], chunk10: 6.704613934127548\n",
      "Gaussian fit parameters for RPC4-10: amplitude = 4.252058677022187, mean = 5.884275589590985, std deviation = 1.181383463338961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data_output/tof_chunks.pdf'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(RTools)\n",
    "RTools.compile_and_plot_tof_chunk(reconstructor.dT,rpc_comparison, 10, pdf_filename='Data_output/tof_chunks.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Peter\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "reconstructor.plot_tof_offset(rpc_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
