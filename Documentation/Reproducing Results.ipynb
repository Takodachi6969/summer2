{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains methods on how to reproduce all the results obtained. This is the quickstart tutorial on how to reproduce all results I have obtained\n",
    "\n",
    "For methods of realignment, see realignment tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All example data used are latest data proAnubis_240716_2142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reconstruction algorithm was adapted to the latest data behavior where noise bursts were seen. The data produced in this notebook are designed to run fast, feel free to remove the ignorance on noise bursts and increase the number of chuncks probed to obtain better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is proAnubis_Analaysis_Tools?\n",
    "\n",
    "proAnubis_Analaysis_Tools is a specialised package for analysing data from the proAnubis setup (https://doi.org/10.22323/1.450.0168) through track reconstruction, time of flight, data quality checks, etc. It works alongside the rawFileReader in Osiris to perform these tasks for TDC event aligned events. \n",
    "\n",
    "Within proAnubis_Analyasis_tools are the Reconstructor, ToF tools and general Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\O'\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_31160\\74488883.py:4: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  sys.path.insert(1, '..\\Osiris Temp\\processing\\python') # Insert your path to Osiris\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "sys.path.insert(1, '..\\Osiris Temp\\processing\\python') # Insert your path to Osiris\n",
    "import Analysis_tools as ATools\n",
    "import rawFileReader\n",
    "import proAnubis_Analysis_Tools\n",
    "import Reconstruction_tools as RTools\n",
    "import mplhep as hep\n",
    "import Timing_tools as TTools\n",
    "import Reconstruction_tools as RTools\n",
    "hep.style.use([hep.style.ATLAS])\n",
    "file_path = '../../Data/proAnubis_240716_2142.raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, These are some useful decorators for certain functions to capture variables if needed. This is better than doing Global variables for clarity and avoid later on mess up certain algorithms. \n",
    "Note that if you are unsure what it does, please read https://realpython.com/primer-on-python-decorators/\n",
    "Please note you need to restart the environment if the function rewrote by the decorator runs into issue, because the function will not return to original state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example on how to get alignment messages, which might come in handy when debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Decorators to capture things in the loop so you don't need to redo any calculations later......\n",
    "def debug_decorator(processedEvents):\n",
    "    def inner_decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            result = func(*args, **kwargs)\n",
    "            insertion_list = result\n",
    "            update = args[0]\n",
    "            if insertion_list != [0,0,0,0,0]:\n",
    "                print(f'New alignment, Event chunk {processedEvents}, insertions {insertion_list}, Updates: {update}')\n",
    "            return result\n",
    "        return wrapper\n",
    "    return inner_decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example event loop on how to use fReader Osiris. Please rewrite the file path to your actual raw file that is not on this github due to the size. You need to put the Data file on your own disk and DO NOT try to commit any raw file on the github. becuase raw files are typically large, and synchronization will time out, but the commit will go into history. Then later on if you want to revert, you will have go back and find the exact commit you did, and remove. Which is a pain in the ass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jupyter notebook doesn't reload your import even when the content of the file is changed. This is crucial\n",
    "importlib.reload(rawFileReader)\n",
    "#The interval is the amount of chunks you want to do realignment analysis with. This also determines the size of the chunk your output is.\n",
    "interval = 100\n",
    "fReader = rawFileReader.fileReader(file_path)\n",
    "# Order is the alignment comparisons you need to specify. for each sublist in order is a pairwise comparison between 2 TDCs.\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] #Your order to the TDC alignment\n",
    "# max_process_event_chunk is used to terminate early in the file reading loop. Your total number of events read will be defined as interval * max_proess_event_chunk\n",
    "max_process_event_chunk = 100\n",
    "# some initialisation to store things\n",
    "processedEvents = 0\n",
    "events = []\n",
    "\n",
    "#Remember the original function\n",
    "original_ConstructEventInsertionList = ATools.ConstructEventInsertionList \n",
    "# condition to end event loop early\n",
    "while processedEvents < max_process_event_chunk:\n",
    "    processedEvents += 1\n",
    "    #Apply your decorator to change the function\n",
    "    ATools.ConstructEventInsertionList  = debug_decorator(processedEvents)(original_ConstructEventInsertionList)\n",
    "    event = fReader.get_aligned_events(order=order, interval=interval)\n",
    "#reset afterwards\n",
    "ATools.ConstructEventInsertionList  = original_ConstructEventInsertionList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can do the calculation twice, this might slow things down, but it is more intuitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 100/100 [00:02<00:00, 45.39Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # reload in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 100 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "\n",
    "#Initialise variables to store the results\n",
    "mets = [[] for tdc in range(5)]\n",
    "empty_header = 0\n",
    "emtpy_events_with_header = [[], []]\n",
    "tdc_event_count = [[] for tdc in range(5)]\n",
    "\n",
    "\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        tdc_event_count_buffer = RTools.find_tdc_event_count(event_chunk)\n",
    "        [tdc_event_count[i].append(tdc_event_count_buffer[i]) for i in range(5)]\n",
    "        for idx, (i, j) in enumerate(order):\n",
    "            x, y, l, m = ATools.find_tdc_alignment_metric(i, j)\n",
    "            alignMet = ATools.calcAvgAlign(event_chunk, 0, x, y, l, m, i, j, processedEvents)\n",
    "            empty_header += (alignMet == 100)\n",
    "            emtpy_events_with_header[0].append(empty_header)\n",
    "            emtpy_events_with_header[1].append(processedEvents)\n",
    "            mets[idx].append(alignMet)\n",
    "        pbar.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_13240\\4261640906.py:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  ax.set_ylabel('Average $\\sqrt{d\\eta^2+d\\phi^2}$')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for idx, item in enumerate(order):\n",
    "    met = mets[idx]\n",
    "    i, j = item\n",
    "    binsx = [x for x in range(len(met))]\n",
    "    ax.plot(binsx, met, label=f'TDC{i} and TDC{j}, offset 0')\n",
    "\n",
    "\n",
    "ax.set_xlim(0, max_process_event_chunk)\n",
    "ax.set_ylim(-1, 40)\n",
    "ax.legend()\n",
    "ax.set_title('Alignment graph')\n",
    "ax.set_ylabel('Average $\\sqrt{d\\eta^2+d\\phi^2}$')\n",
    "ax.set_xlabel('Processed Event chunks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(emtpy_events_with_header[1], emtpy_events_with_header[0], marker='o')\n",
    "plt.xlabel('Processed Events')\n",
    "plt.ylabel('Only Header')\n",
    "plt.title('Only Header vs Processed Events')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for tdc in range(5):\n",
    "    met = tdc_event_count[tdc]\n",
    "    binsx = [x for x in range(len(met))]\n",
    "    ax.plot(binsx, met, label=f'TDC{tdc}')\n",
    "\n",
    "ax.set_xlim(0, max_process_event_chunk)\n",
    "# ax.set_ylim(-1, 100)\n",
    "ax.legend()\n",
    "ax.set_title('TDC number of events recorded')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Processed Event')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\OneDrive - University of Cambridge\\Desktop\\summer2\\Documentation\\proAnubis_Analysis_Tools.py:316: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  thisHist.cbar.set_label('$\\eta$ hit time - $\\phi$ hit time, Average (ns)', rotation=270, y=0.3, labelpad=23)\n",
      "c:\\Users\\Peter\\OneDrive - University of Cambridge\\Desktop\\summer2\\Documentation\\proAnubis_Analysis_Tools.py:336: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  thisHist.cbar.set_label('$\\eta$ hit time - $\\phi$ hit time, Average (ns)', rotation=270, y=0.3, labelpad=23)\n",
      "Processing Events: 100%|██████████| 150/150 [00:02<00:00, 51.22Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 150 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "TAnalyser = proAnubis_Analysis_Tools.Timing_Analyser(initial_event_chunk,processedEvents)\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        if event_chunk:\n",
    "            TAnalyser.update_event(event_chunk, processedEvents)\n",
    "            status, failure = TAnalyser.check_eta_trigger()\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_counts_windows = [len(TAnalyser.count[count]) for count in range(7)]\n",
    "total_windows = sum(event_counts_windows)\n",
    "normalized_windows = [count / total_windows for count in event_counts_windows]\n",
    "normalized_windows.append(normalized_windows[-1])\n",
    "plt.figure(figsize=(12, 8))\n",
    "r1 = list(range(7)) + [6.5]\n",
    "\n",
    "# plt.step(r1, normalized_linux, color='mediumseagreen', linestyle='-', linewidth=2, markersize=6, label='0-15000', alpha=1, where='mid')\n",
    "plt.step(r1, normalized_windows, color='dodgerblue', linestyle='-', linewidth=2, markersize=6, label='0-15000?', alpha=1, where='mid')\n",
    "\n",
    "plt.xlabel('Trigger Number')\n",
    "plt.ylabel('Number of Events Normalized')\n",
    "plt.title('Normalized Event Count')\n",
    "plt.ylim(0)\n",
    "plt.xlim(0, 6.5)\n",
    "plt.xticks(range(7))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAnalyser.plot_rpc_involvement_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reconstruction event loop demonstration for finding the efficiency of each RPCs. Effiiency was calculated using a tag and probe method, where the test RPC is removed, and a track using all other 5 RPCs were fitted. The fitted track is then extrapolated to the test RPC, where the efficiency is probed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events:   0%|          | 0/200 [00:00<?, ?Events/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 200/200 [00:03<00:00, 63.83Events/s] \n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 200 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "tdc_mets = [[] for tdc in range(5)]\n",
    "Tot_TDC_info = [[] for tdc in range(5)]\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk, tdc_met, TDC_info = fReader.get_aligned_events(order=order, interval=interval, extract_tdc_mets = True)\n",
    "        [tdc_mets[i].append(tdc_met[i]) for i in range(5) if tdc_met[i] != 0]\n",
    "        [Tot_TDC_info[i].extend(TDC_info[i]) for i in range(5) if TDC_info[i]]\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for tdc in range(5):\n",
    "    met = tdc_mets[tdc]\n",
    "    binsx = [x * 25 for x in range(len(met))]\n",
    "    ax.plot(binsx,met, label = f'tdc {tdc}', color = colors[tdc])\n",
    "\n",
    "ax.set_xlim(0,max_process_event_chunk)\n",
    "# ax.set_ylim(0,1)\n",
    "ax.legend()\n",
    "ax.set_title('TDC monitoring metric')\n",
    "ax.set_ylabel('bad time behavior / nominal time behavior')\n",
    "ax.set_xlabel('processed Event')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(TTools)\n",
    "TTools.plot_tdc_error_times(Tot_TDC_info)\n",
    "TTools.plot_tdc_error_times_custom_ranges(Tot_TDC_info, [(0, 100), (100, 200)], output_pdf='Data_output/TDC_first_hit_time.pdf')\n",
    "TTools.plot_tdc_error_channels(Tot_TDC_info)\n",
    "TTools.plot_tdc_error_channels_custom_ranges(Tot_TDC_info, [(0, 100), (100, 200)], tdcs_to_plot=None, output_pdf='Data_output/TDC_first_hit_time_channel.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capturer:\n",
    "    def __init__(self):\n",
    "        self.TDC_alignment_time = [[] for _ in range(5)]\n",
    "        self.processedEvents = 0 \n",
    "\n",
    "    def extra_calculation_decorator(self, func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            minTimes = [300, 300]\n",
    "            minChans = [-1, -1]\n",
    "            minRPC = [-1, -1]\n",
    "            minWord = [-1, -1]\n",
    "            tdc = [-1, -1]\n",
    "            eta = kwargs.get('eta', True)\n",
    "            \n",
    "            rpc1Hits = args[0]\n",
    "            rpc2Hits = args[1]\n",
    "            skipChans = kwargs.get('skipChans', [])\n",
    "\n",
    "            # Perform the core function\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            if result == -1:\n",
    "                return result\n",
    "            print(rpc1Hits)\n",
    "            print(rpc2Hits)\n",
    "            for hit in rpc1Hits:\n",
    "                if hit.time < minTimes[0] and hit.channel not in skipChans:\n",
    "                    minTimes[0] = hit.time\n",
    "                    minChans[0] = hit.channel\n",
    "                    minRPC[0] = hit.rpc\n",
    "            for hit in rpc2Hits:\n",
    "                if hit.time < minTimes[1] and hit.channel not in skipChans:\n",
    "                    minTimes[1] = hit.time\n",
    "                    minChans[1] = hit.channel\n",
    "                    minRPC[1] = hit.rpc\n",
    "            \n",
    "            if -1 in minChans:\n",
    "                return -1\n",
    "            \n",
    "            # Assuming rpcHitToTdcChan is a function available in the context\n",
    "            a = TTools.rpcHitToTdcChan(minRPC[0], minChans[0], eta)\n",
    "            b = TTools.rpcHitToTdcChan(minRPC[1], minChans[1], eta)\n",
    "            \n",
    "            tdc[0], minWord[0] = a\n",
    "            tdc[1], minWord[1] = b\n",
    "\n",
    "            # Update the TDC_alignment_time attribute\n",
    "            self.TDC_alignment_time[tdc[0]].append((minTimes[0], a, self.processedEvents))\n",
    "            self.TDC_alignment_time[tdc[1]].append((minTimes[1], b, self.processedEvents))\n",
    "\n",
    "            return result\n",
    "\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_testAlign = ATools.testAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATools.testAlign = original_testAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Analysis_tools.rpcHit object at 0x000001DB32B141D0>]\n",
      "[<Analysis_tools.rpcHit object at 0x000001DB32B151F0>]\n",
      "[<Analysis_tools.rpcHit object at 0x000001DB32B17530>, <Analysis_tools.rpcHit object at 0x000001DB32B17590>, <Analysis_tools.rpcHit object at 0x000001DB32B16F90>, <Analysis_tools.rpcHit object at 0x000001DB32B15580>]\n",
      "[<Analysis_tools.rpcHit object at 0x000001DB32B146E0>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m max_process_event_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# End the loop early\u001b[39;00m\n\u001b[0;32m     15\u001b[0m processedEvents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Initialization\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m initial_event_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mfReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_aligned_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m tdc_mets \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m tdc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)]\n\u001b[0;32m     18\u001b[0m Tot_TDC_info \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m tdc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\Peter\\OneDrive - University of Cambridge\\Desktop\\summer2\\Documentation\\..\\Osiris Temp\\processing\\python\\rawFileReader.py:64\u001b[0m, in \u001b[0;36mfileReader.get_aligned_events\u001b[1;34m(self, order, interval, extract_tdc_mets)\u001b[0m\n\u001b[0;32m     62\u001b[0m             i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdc_monitoring_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 64\u001b[0m aligned, realigned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoRealign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevts_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_alignment_status(aligned, realigned) \n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_adjustment_window(realigned)\n",
      "File \u001b[1;32mc:\\Users\\Peter\\OneDrive - University of Cambridge\\Desktop\\summer2\\Documentation\\..\\Osiris Temp\\processing\\python\\rawFileReader.py:87\u001b[0m, in \u001b[0;36mfileReader.doRealign\u001b[1;34m(self, event_chunk, order, skipChans)\u001b[0m\n\u001b[0;32m     85\u001b[0m i, j \u001b[38;5;241m=\u001b[39m item\n\u001b[0;32m     86\u001b[0m x, y, l, m \u001b[38;5;241m=\u001b[39m aTools\u001b[38;5;241m.\u001b[39mfind_tdc_alignment_metric(i, j)\n\u001b[1;32m---> 87\u001b[0m alignMet \u001b[38;5;241m=\u001b[39m \u001b[43maTools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalcAvgAlign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffSet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtdc1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtdc0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessedEvents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipChans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipChans\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#ProcessedEvents is required for class object RPCHit. I know its redundency, but hard to fix\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alignMet \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m15\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m alignMet \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[0;32m     89\u001b[0m     aligned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\/Users//Peter//OneDrive - University of Cambridge//Desktop//summer2//Osiris Temp//processing//python\\Analysis_tools.py:155\u001b[0m, in \u001b[0;36mcalcAvgAlign\u001b[1;34m(event_chunk, offSet, i, j, k, l, tdc1, tdc0, processedEvents, skipChans)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:  \n\u001b[0;32m    154\u001b[0m     etOff \u001b[38;5;241m=\u001b[39m testAlign(etaHits[i],etaHits[j], skipChans \u001b[38;5;241m=\u001b[39m skipChans)\n\u001b[1;32m--> 155\u001b[0m     phOff \u001b[38;5;241m=\u001b[39m \u001b[43mtestAlign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphiHits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mphiHits\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipChans\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mskipChans\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m etOff\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m phOff\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    157\u001b[0m         mets\u001b[38;5;241m.\u001b[39mappend(math\u001b[38;5;241m.\u001b[39msqrt(etOff\u001b[38;5;241m*\u001b[39metOff\u001b[38;5;241m+\u001b[39mphOff\u001b[38;5;241m*\u001b[39mphOff))\n",
      "Cell \u001b[1;32mIn[25], line 44\u001b[0m, in \u001b[0;36mCapturer.extra_calculation_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m a \u001b[38;5;241m=\u001b[39m TTools\u001b[38;5;241m.\u001b[39mrpcHitToTdcChan(minRPC[\u001b[38;5;241m0\u001b[39m], minChans[\u001b[38;5;241m0\u001b[39m], eta)\n\u001b[0;32m     42\u001b[0m b \u001b[38;5;241m=\u001b[39m TTools\u001b[38;5;241m.\u001b[39mrpcHitToTdcChan(minRPC[\u001b[38;5;241m1\u001b[39m], minChans[\u001b[38;5;241m1\u001b[39m], eta)\n\u001b[1;32m---> 44\u001b[0m tdc[\u001b[38;5;241m0\u001b[39m], minWord[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m     45\u001b[0m tdc[\u001b[38;5;241m1\u001b[39m], minWord[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m b\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Update the TDC_alignment_time attribute\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "\n",
    "Capturer = Capturer()\n",
    "\n",
    "# Apply the decorator\n",
    "original_testAlign = ATools.testAlign\n",
    "ATools.testAlign = Capturer.extra_calculation_decorator(ATools.testAlign)\n",
    "\n",
    "# Main loop\n",
    "interval = 100  # Set your monitoring chunk size\n",
    "fReader = rawFileReader.fileReader(file_path)  # Load in the class object\n",
    "order = [[0, 1], [1, 2], [2, 3], [3, 4]]  # Order what you want to align\n",
    "max_process_event_chunk = 1000  # End the loop early\n",
    "processedEvents = 0  # Initialization\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "tdc_mets = [[] for tdc in range(5)]\n",
    "Tot_TDC_info = [[] for tdc in range(5)]\n",
    "\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Restore the original function\n",
    "ATools.testAlign = original_testAlign\n",
    "\n",
    "# Process or print the TDC_alignment_time as needed\n",
    "# print(tdc_manager.TDC_alignment_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTTools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_tdc_alignment_channels_separate_ranges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtdc_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTDC_alignment_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Peter\\OneDrive - University of Cambridge\\Desktop\\summer2\\Documentation\\Timing_tools.py:377\u001b[0m, in \u001b[0;36mplot_tdc_alignment_channels_separate_ranges\u001b[1;34m(TDC_alignment_time, tdcs_to_plot)\u001b[0m\n\u001b[0;32m    374\u001b[0m bad_channels_dict_25000_40000 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m min_time, min_word, processed_events \u001b[38;5;129;01min\u001b[39;00m TDC_alignment_time[tdc]:\n\u001b[1;32m--> 377\u001b[0m     channel \u001b[38;5;241m=\u001b[39m \u001b[43mmin_word\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m processed_events \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m15000\u001b[39m:\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m min_time \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "TTools.plot_tdc_alignment_channels_separate_ranges(tdc_manager.TDC_alignment_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 100 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "TAnalyser = proAnubis_Analysis_Tools.Timing_Analyser(initial_event_chunk,processedEvents)\n",
    "while processedEvents < max_process_event_chunk:\n",
    "    processedEvents += 1\n",
    "    event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "    if event_chunk:\n",
    "        TAnalyser.update_event(event_chunk, processedEvents)\n",
    "        TAnalyser.readTDCTimeDiffs()\n",
    "        \n",
    "outDict = {'totDiffs':TAnalyser.totDiffs,\n",
    "                    'nDiffs':TAnalyser.nDiffs,\n",
    "                    'diffHists':TAnalyser.diffHists} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n",
      "Warning: <class 'uhi.numpy_plottable.NumPyPlottableHistogram'> is not allowed to get flow bins, flow bin option set to None\n"
     ]
    }
   ],
   "source": [
    "residEta, residPhi = TAnalyser.Calculate_Residual_and_plot_TDC_Time_Diffs(outDict, \n",
    "                                                     pdf_filename='Data_output/TDC_time_diffs.pdf', \n",
    "                                                     max_itr = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\e'\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_13240\\2682300778.py:13: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  ax.set_xlabel('$\\phi$ Channel')\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_13240\\2682300778.py:28: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  ax.set_xlabel('$\\eta$ Channel')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rpcNames = {0:\"Triplet Low\",1: \"Triplet Mid\", 2:\"Triplet Top\", 3:\"Singlet\",4:\"Doublet Low\",5:\"Doublet Top\"}\n",
    "fig, ax = plt.subplots(1, figsize=(16, 8), dpi=100)\n",
    "phichannels = [x-0.5 for x in range(65)]\n",
    "\n",
    "for idx, rpc in enumerate([0,1,2,3,4,5]):\n",
    "    plotPhiResids = residPhi[idx].copy()\n",
    "    plotPhiResids.append(plotPhiResids[-1])\n",
    "    plt.step(phichannels,plotPhiResids,linewidth=3,label=rpcNames[rpc],where='post')\n",
    "yrange = ax.get_ylim()\n",
    "#ax.text(40, 0.8*yrange[1], \"Slope: \"+str(round(popt[0],3))+\" ns/channel, Offset: \"+str(round(popt[1],2))+\" ns\", fontsize=14,\n",
    "#               verticalalignment='top')\n",
    "ax.set_xlabel('$\\phi$ Channel')\n",
    "ax.set_ylabel('Time Residual (ns)')\n",
    "# ax.set_ylim([-6,6])\n",
    "ax.set_xlim([-0.5,63.5])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig, ax = plt.subplots(1, figsize=(16, 8), dpi=100)\n",
    "etchannels = [x-0.5 for x in range(33)]\n",
    "for idx, rpc in enumerate([0,1,2,3,4,5]):\n",
    "    plotEtaResids = residEta[idx].copy()\n",
    "    plotEtaResids.append(plotEtaResids[-1])\n",
    "    plt.step(etchannels,plotEtaResids,linewidth=3,label=rpcNames[rpc],where='post')\n",
    "yrange = ax.get_ylim()\n",
    "#ax.text(40, 0.8*yrange[1], \"Slope: \"+str(round(popt[0],3))+\" ns/channel, Offset: \"+str(round(popt[1],2))+\" ns\", fontsize=14,\n",
    "#               verticalalignment='top')\n",
    "ax.set_xlabel('$\\eta$ Channel')\n",
    "ax.set_ylabel('Time Residual (ns)')\n",
    "# ax.set_ylim([-6,6])\n",
    "ax.set_xlim([-0.5,31.5])\n",
    "plt.legend()\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 100 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "reconstructor = proAnubis_Analysis_Tools.Reconstructor(initial_event_chunk, processedEvents)\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        #Zone of Reconstruction\n",
    "        if event_chunk:\n",
    "            reconstructor.update_event(event_chunk, processedEvents)\n",
    "            reconstructor.populate_hits()\n",
    "            reconstructor.apply_systematic_correction(residEta, residPhi)\n",
    "            cluster = reconstructor.make_cluster()\n",
    "            reconstructor.reconstruct_and_extrapolate(cluster)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "for RPC in range(6):\n",
    "    if reconstructor.possible_reconstructions[RPC] == 0:\n",
    "        efficiency = [0 for x in reconstructor.successful_reconstructions[RPC]]\n",
    "    else:\n",
    "        efficiency = [x / reconstructor.possible_reconstructions[RPC] for x in reconstructor.successful_reconstructions[RPC]]\n",
    "    plt.plot(reconstructor.tol, efficiency, label=f'RPC {RPC}')\n",
    "\n",
    "plt.xlabel('Tolerance')\n",
    "plt.ylabel('Efficiency')\n",
    "plt.title('idc what the title is')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(reconstructor.possible_reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "rpcNames = {0: \"Triplet Low\", 1: \"Triplet Mid\", 2: \"Triplet Top\", 3: \"Singlet\", 4: \"Doublet Low\", 5: \"Doublet Top\"}\n",
    "\n",
    "success_events = [[0 for etchan in range(32)] for phchan in range(64)]\n",
    "\n",
    "with PdfPages('Data_output/reconstruction_heatmap_plots.pdf') as pdf:\n",
    "    for rpc in range(6):\n",
    "        for ph in range(64):\n",
    "            for et in range(32):\n",
    "                if reconstructor.successful_reconstructed_coords[rpc][ph][et] > 0:\n",
    "                    total_successful = reconstructor.successful_reconstructed_coords[rpc][ph][et]\n",
    "                    total_events = reconstructor.possible_reconstructions_coords[rpc][ph][et]\n",
    "                    if total_events > 0:\n",
    "                        success_events[ph][et] = total_successful / total_events\n",
    "                    else:\n",
    "                        success_events[ph][et] = 0  # No events, efficiency is 0\n",
    "\n",
    "        fig, ax = plt.subplots(1, figsize=(16, 8), dpi=100)\n",
    "        etachannels = [x - 0.5 for x in range(33)]\n",
    "        phichannels = [x - 0.5 for x in range(65)]\n",
    "        etaHist = (success_events, np.array(phichannels), np.array(etachannels))\n",
    "        zrange = [0, max(max(row) for row in success_events)]\n",
    "        thisHist = hep.hist2dplot(etaHist, norm=colors.Normalize(zrange[0], zrange[1]))\n",
    "        thisHist.cbar.set_label('Successful reconstructions / Possible reconstructions', rotation=270, y=0.3, labelpad=23)\n",
    "        plt.ylim(31.5, -0.5)\n",
    "        plt.ylabel(\"Eta Channel\")\n",
    "        plt.xlabel(\"Phi Channel\")\n",
    "        ax.set_title(rpcNames[rpc])\n",
    "\n",
    "        # Draw lines\n",
    "        x_points = [-0.5, 64.5]\n",
    "        y_points = [7.5, 15.5, 23.5]\n",
    "        for y_point in y_points:\n",
    "            plt.plot(x_points, [y_point, y_point], 'k', linestyle='dotted')\n",
    "        y_points = [-0.5, 31.5]\n",
    "        x_points = [7.5, 15.5, 23.5, 31.5, 39.5, 47.5, 55.5]\n",
    "        for x_point in x_points:\n",
    "            plt.plot([x_point, x_point], y_points, 'k', linestyle='dashed')\n",
    "\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"PDF created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining results from timing analysis to the reconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\OneDrive - University of Cambridge\\Desktop\\summer2\\Documentation\\proAnubis_Analysis_Tools.py:316: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  for y_point in y_points_dotted:\n",
      "c:\\Users\\Peter\\OneDrive - University of Cambridge\\Desktop\\summer2\\Documentation\\proAnubis_Analysis_Tools.py:336: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  plt.plot([x_point, x_point], y_points_dashed, 'k', linestyle='dashed')\n",
      "Processing Events: 100%|██████████| 150/150 [00:19<00:00,  7.62Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "fReader = rawFileReader.fileReader(file_path) # load in the classs object\n",
    "order = [[0,1], [1,2], [2,3], [3,4]] # Order what you want to align\n",
    "max_process_event_chunk = 150 # End the loop early\n",
    "processedEvents = 0 # Initialisation\n",
    "initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "reconstructor = proAnubis_Analysis_Tools.Reconstructor(initial_event_chunk, processedEvents, tof_correction=True)\n",
    "TAnalyser = proAnubis_Analysis_Tools.Timing_Analyser(initial_event_chunk,processedEvents)\n",
    "with tqdm(total=max_process_event_chunk, desc=\"Processing Events\", unit='Events') as pbar:\n",
    "    while processedEvents < max_process_event_chunk:\n",
    "        processedEvents += 1\n",
    "        event_chunk = fReader.get_aligned_events(order=order, interval=interval)\n",
    "        if event_chunk:\n",
    "            reconstructor.update_event(event_chunk, processedEvents)\n",
    "            # if processedEvents < 250:\n",
    "            #     pbar.update(1)\n",
    "            #     continue\n",
    "            reconstructor.populate_hits()\n",
    "            reconstructor.apply_systematic_correction(residEta, residPhi)\n",
    "            cluster = reconstructor.make_cluster()\n",
    "            filtered_events = RTools.filter_events(cluster,1,6)     \n",
    "            reconstructor.extract_angles_phi_eta_timed_DZ_modified(filtered_events)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(14, 20))\n",
    "bin_edges = np.arange(-90.5, 91.5, 1)\n",
    "phi_edges = np.arange(-180.5, 181.5, 1)\n",
    "ax1.bar(bin_edges[:-1], reconstructor.eta_histogram, width=1, edgecolor='black', align='edge')\n",
    "ax1.set_title('eta Angles Histogram chunk3')\n",
    "ax1.set_xlabel('eta Angle (degrees)')\n",
    "ax1.set_ylabel('Counts')\n",
    "\n",
    "ax2.bar(bin_edges[:-1], reconstructor.phi_histogram, width=1, edgecolor='black', align='edge')\n",
    "ax2.set_title('phi Angles Histogram chunk3')\n",
    "ax2.set_xlabel('phi Angle (degrees)')\n",
    "ax2.set_ylabel('Counts')\n",
    "\n",
    "ax3.bar(phi_edges[:-1], reconstructor.solid_theta_histogram, width=1, edgecolor='black', align='edge')\n",
    "ax3.set_title('solid theta Angles Histogram chunk3')\n",
    "ax3.set_xlabel('solid theta Angle (degrees)')\n",
    "ax3.set_ylabel('Counts')\n",
    "\n",
    "ax4.bar(phi_edges[:-1], reconstructor.solid_phi_histogram, width=1, edgecolor='black', align='edge')\n",
    "ax4.set_title('solid phi Angles Histogram chunk3')\n",
    "ax4.set_xlabel('solid phi Angle (degrees)')\n",
    "ax4.set_ylabel('Counts')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
